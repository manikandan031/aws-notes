**S3**

[back](index.md)

* Buckets store objects
* Bucket names must be globally unique across regions and users.
* S3 is a regional service even though it looks like global
* Naming convention
    * No uppercase, no underscore
    * 3-63 chars long
    * Should not be an ip
    * Should start with lowercase or number
* Objects have a key - There are no directories in S3 even though S3 makes it look like there are directories
    * S3://my-bucket/my-dir/file.txt
    * my-dir/file.txt is the key
* Max object size is 5TB. At a time we can upload 5GB. For larger files, divide the files into smaller chunks and upload using multipart upload. Multipart is recommended for files more than 100mb.
* Objects can be versioned if versioning is enabled at bucket level.
* Objects can be exposed using http/https endpoints.
* Pre signed url - can be used by authorised user for a limited time access
* Public object url - Will work only if object is publicly accessible
* Encryption
    * SSE-S3 - using keys managed by AWS
        * Must set header “x-amz-server-side-encryption” : “AES256"
    * SSE-KMS - uses AWS key management store
        * Must set header “x-amz-server-side-encryption” : "aws:kms"
    * SSE-C - user manages the encryption keys and not stored by aws
        * Https is mandatory
        * Encryption key is provided in the header for every request.
    * Client side encryption
        * Use aws client library to encrypt before uploading to s3
* Security
    * IAM policy
    * Bucket policy
        * Can be used to set up access across aws accounts as well.
    * A principal can access a resource if either the IAM policy allows OR the bucket policy allows.
    * Explicit deny in IAM policy takes precedence over bucket policy
* Block public access - can be set at bucket level as well as account level. Should be disabled for static web hosting.
* Static website url format - <bucket-name>.s3-website-<aws-region>.amazonaws.com
    * Should add a bucket policy to allow GET* for all principals
* CORS
    * Origin - contains host, protocol and port. Any one changes its a cross origin.
    * To allow CORS requests the other origin must explicitly allow the main origin.
    * There will be a preflight request sent to the cross origin to check if the access is allowed by the main origin.
    * Preflight response
        * Access-control-allow-origin: example.com
        * Access-control-allowed-methods: GET, PUT, DELETE

* MFA Delete
    * To protect from accidental permanent deletion
    * Should enable versioning
    * Can enable/disable MFA delete for permanent deletion of objects only via CLI
* S3 Access Logs
    * Access logs can be turned on and stored in another bucket
    * Note: If the target bucket is mentioned as the source bucket itself it will result in a log loop and a huge bill.
    * AWS Athena can be used to analyse S3 access logs to get reports like how many 404 request etc..
* Replication
    * Must enable versioning
    * CRR (Cross Region Replication) and SRR(Same Region Replication) is possible
    * Replication can be done to other aws accounts as well
    * IAM policy must have permissions
* Storage Classes
    * S3 Standard
    * S3 Infrequent Accèss (IA)
        * Slightly less availability, less cost, can GET immediately
        * Used for storing backups 
    * S3 One Zone IA
        * Even lesser availability and durability as data is present in only 1 zone. 
        * Used for storing data that can be recreated like thumbnails.
    * S3 intelligent Tiering
        * Automatically moves objects between standard and IA classes
        * Small cost for monitoring and moving
    * Glacier
        * Used to store archived data for long periods and that is accessed less frequently
        * Access modes
            * Expedited (5-10 mins)
            * Standard (3-5hrs)
            * Bulk (5-12Hrs)
        * Minimum 90 days
        * Storage cost is cheaper, retrieval cost is higher if retrieved in expedited mode.
    * Glacier Deep Archive
        * Even less cost for storage. 
        * Data can be accessed only after 48 Hrs
        * Minimum 180 days
    * Lifecycle rules can we configured to automatically move objects from one storage class to another after specific time intervals. Once moved to a higher level, object cannot be moved back.
    * S3 analytics can be enabled to generate reports which can be used as insight to set up lifecycle rules
    * Performance
        * Typically 100-200ms
        * Can handle 3500 PUT/POST/DELETE, 5500 GET/HEAD requests per second per prefix in a bucket
        * If encrypted using KMS, KMS will impact performance for encryption and decryption
        * Transfer acceleration 
            * Transfers file to edge location via internet
            * From edge location to target region in was private network
        * Byte Range Fetch can be used to get specific byte range from objects
    * S3 Select and Glacier select can be used to run basic SQL queries against files. No aggregation can be performed. For advanced use cases, use Athena.
    * S3 event notifications can be configured to - run lambda, SNS topic, SQS queue etc. For example, generate thumbnail for images uploaded to S3.
    * S3 Requester Pays - we can configure a bucket for requester pays in which the requester will pay for network cost. The owner will pay for the storage.
    * S3 Lock policies and Glacier Vault Lock - can be used to ensure no one can delete objects from a bucket even the root user. Used for compliance purposed.

[back](index.md)
